{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Aim</b>\n",
    "\n",
    "Train a model to classify comments as positive or negative\n",
    "\n",
    "<br><b>Background</b>\n",
    "\n",
    "The online store \"WikiShop\" is launching a new service. Now, users can edit and supplement product descriptions, much like in wiki communities. This means that customers can suggest their edits and comment on the changes of others. The store needs a tool that will identify toxic comments and forward them for moderation.\n",
    "\n",
    "Client's priorities:\n",
    "\n",
    "- The F1 quality metric should be no less than 0.75.\n",
    "\n",
    "<br><b>Data Description</b>\n",
    "\n",
    "Features:\n",
    "\n",
    "- *text* — comment text\n",
    "\n",
    "Target Feature:\n",
    "\n",
    "- *toxic* — 0: non-toxic comment; 1: toxic comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install catboost\n",
    "#!pip install lightgbm\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install spacy\n",
    "#!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Игорь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from scipy.sparse import vstack\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "import warnings\n",
    "nltk.download('stopwords')\n",
    "warnings.simplefilter(\"ignore\")\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    df = pd.read_csv('toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "text          0\n",
       "toxic         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that only two classes exist. This will also highlight any class imbalance\n",
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of class 0 objects: 0.8983878663084147\n",
      "Proportion of class 1 objects: 0.10161213369158527\n"
     ]
    }
   ],
   "source": [
    "# check for class imbalance\n",
    "\n",
    "print('Proportion of class 0 objects:', len(df.loc[df['toxic'] == 0])/len(df.loc[df['toxic']]))\n",
    "print('Proportion of class 1 objects:', len(df.loc[df['toxic'] == 1])/len(df.loc[df['toxic']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a pronounced class imbalance detected. It will be necessary to address this imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the spacy model\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into features and the target feature\n",
    "\n",
    "features = df['text']\n",
    "target = df['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function for lemmatization and cleaning of comments\n",
    "\n",
    "def space_lemmatize_clear(text):\n",
    "    text = re.sub(r'[^a-zA-Z ]',' ', text)    \n",
    "    text = text.split()\n",
    "    doc = nlp(\" \".join(text))\n",
    "    return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to the features\n",
    "\n",
    "features_lemma = features.apply(space_lemmatize_clear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe with lemmatized features\n",
    "\n",
    "df_lemma=pd.concat([features_lemma,target],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into three sets: training, validation, and testing\n",
    "\n",
    "df_train, df_valid_and_test = train_test_split(df_lemma, test_size=0.4, \n",
    "                                               random_state=12345, stratify=target)\n",
    "\n",
    "df_valid_and_test_features = df_valid_and_test.drop(['toxic'], axis=1)\n",
    "df_valid_and_test_target = df_valid_and_test['toxic']\n",
    "df_valid, df_test = train_test_split(df_valid_and_test, test_size=0.5, \n",
    "                                     random_state=12345, stratify=df_valid_and_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the set of features and the target feature for each set:\n",
    "\n",
    "# training\n",
    "features_train = df_train['text']\n",
    "target_train = df_train['toxic']\n",
    "\n",
    "# validation\n",
    "features_valid = df_valid['text']\n",
    "target_valid = df_valid['toxic']\n",
    "\n",
    "# testing\n",
    "features_test = df_test['text']\n",
    "target_test = df_test['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify stop words and create a counter\n",
    "\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the features\n",
    "\n",
    "train_tf_idf = count_tf_idf.fit_transform(features_train)\n",
    "valid_tf_idf = count_tf_idf.transform(features_valid)\n",
    "test_tf_idf = count_tf_idf.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling Function\n",
    "\n",
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "    features_upsampled = vstack([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = np.concatenate([target_zeros] + [target_ones] * repeat)\n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345)\n",
    "    return features_upsampled, target_upsampled\n",
    "\n",
    "features_upsampled, target_upsampled = upsample(train_tf_idf, target_train, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the LR model with upsampling: 0.7722943722943723\n"
     ]
    }
   ],
   "source": [
    "# training and prediction of the LR model with upsampling\n",
    "\n",
    "model_lr = LogisticRegression(random_state=12345, solver='liblinear', max_iter=1000, C=5)\n",
    "model_lr.fit(features_upsampled, target_upsampled)\n",
    "predicted_valid = model_lr.predict(valid_tf_idf)\n",
    "lr_f1_up = f1_score(target_valid, predicted_valid)\n",
    "print(\"F1 score of the LR model with upsampling:\", lr_f1_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling Function\n",
    "\n",
    "def downsample(features, target, fraction):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    random_indices = np.random.choice(features_zeros.shape[0], size=int(features_zeros.shape[0] * fraction), replace=False)\n",
    "    features_zeros_downsampled = features_zeros[random_indices, :]\n",
    "    target_zeros_downsampled = target_zeros.iloc[random_indices]\n",
    "\n",
    "    features_downsampled = vstack([features_zeros_downsampled, features_ones])\n",
    "    target_downsampled = np.concatenate([target_zeros_downsampled, target_ones])\n",
    "\n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=12345)\n",
    "\n",
    "    return features_downsampled, target_downsampled\n",
    "\n",
    "features_downsampled, target_downsampled = downsample(train_tf_idf, target_train, 0.11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the LR model with downsampling: 0.7043360766490604\n"
     ]
    }
   ],
   "source": [
    "# training and prediction of the LR model with downsampling\n",
    "\n",
    "model = LogisticRegression(random_state=12345, solver='liblinear', max_iter=1000, C=5)\n",
    "model.fit(features_downsampled, target_downsampled)\n",
    "predicted_valid = model.predict(valid_tf_idf)\n",
    "lr_f1_down = f1_score(target_valid, predicted_valid)\n",
    "print(\"F1 score of the LR model with downsampling:\", lr_f1_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>\n",
    "\n",
    "Key takeaways from this stage: class imbalance was identified, the best \"balancing\" model was determined to be upsampling. The original sample was divided into training, validation, and testing sets for subsequent training and model testing. Text lemmatization was carried out, and the samples were converted to vector form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the best Decision Tree model on the validation set: 0.6347107438016529 in max_depth: 18\n"
     ]
    }
   ],
   "source": [
    "# training a Decision Tree\n",
    "\n",
    "best_model_dt = None\n",
    "best_result_dt_f1 = 0\n",
    "best_depth_dt = 0\n",
    "for depth in range(1, 20):\n",
    "    model_dt = DecisionTreeClassifier(random_state=12345, max_depth=depth)\n",
    "    model_dt.fit(features_upsampled, target_upsampled)\n",
    "    predictions_dt_valid = model_dt.predict(valid_tf_idf)\n",
    "    f1_dt = f1_score(target_valid, predictions_dt_valid)\n",
    "    if f1_dt > best_result_dt_f1:\n",
    "        best_model_dt = model_dt\n",
    "        best_result_dt_f1 = f1_dt\n",
    "        best_depth_dt = depth\n",
    "\n",
    "print('F1 score of the best Decision Tree model on the validation set:', \n",
    "      best_result_dt_f1, 'in max_depth:', best_depth_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [07:37<00:00, 24.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the best Random Forest model on the validation set: 0.3455190882481161 in max_depth: 19 and n_estimators: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training a Random Forest\n",
    "\n",
    "best_model_rf = None\n",
    "best_result_rf_f1 = 0\n",
    "best_depth_rf = 0\n",
    "best_est_rf = 0\n",
    "for est in tqdm(range(1,20)):\n",
    "    for depth in range(1, 20):\n",
    "        model_rf = RandomForestClassifier(random_state=12345, n_estimators=est, max_depth=depth)\n",
    "        model_rf.fit(features_upsampled, target_upsampled)\n",
    "        prediction_rf_valid = model_rf.predict(valid_tf_idf)\n",
    "        f1_rf = f1_score(target_valid, prediction_rf_valid)\n",
    "        if f1_rf > best_result_rf_f1:\n",
    "            best_model_rf = model_rf\n",
    "            best_result_rf_f1 = f1_rf\n",
    "            best_depth_rf = depth\n",
    "            best_est_rf = est\n",
    "\n",
    "print('F1 score of the best Random Forest model on the validation set:', best_result_rf_f1, \n",
    "      'in max_depth:', best_depth_rf, 'and n_estimators:', best_est_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "F1 score of the CatBoost model on the validation set: 0.7352006930407162\n"
     ]
    }
   ],
   "source": [
    "# creating a CatBoostRegressor model\n",
    "catboost_model = CatBoostClassifier(random_seed=12345, loss_function='Logloss', verbose=False)\n",
    "\n",
    "# hyperparameter range for the CatBoostRegressor model\n",
    "param_grid = {\n",
    "    'learning_rate': [0.03, 0.1],\n",
    "    'depth': [6, 8],\n",
    "    'l2_leaf_reg': [1, 3],\n",
    "    'iterations': [500, 1000],\n",
    "}\n",
    "\n",
    "# Creating a GridSearchCV object\n",
    "f1 = make_scorer(f1_score)\n",
    "grid_search_cb = GridSearchCV(catboost_model, param_grid, cv=3, scoring=f1, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Training the model with the best hyperparameter optimization\n",
    "grid_search_cb.fit(features_upsampled, target_upsampled)\n",
    "best_catboost_model = grid_search_cb.best_estimator_\n",
    "best_catboost_model.fit(features_upsampled, target_upsampled)\n",
    "predictions_catboost_valid = best_catboost_model.predict(valid_tf_idf)\n",
    "f1_catboost_valid = f1_score(target_valid, predictions_catboost_valid)\n",
    "print(\"F1 score of the CatBoost model on the validation set:\", f1_catboost_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Summary</b>\n",
    "\n",
    "In total, we trained four models: Logistic Regression, Decision Tree, Random Forest, and Cat Boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary table:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>Success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.772000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTree</th>\n",
       "      <td>0.634711</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>0.345519</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CatBoost</th>\n",
       "      <td>0.735201</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          F1  Success\n",
       "LogisticRegression  0.772000     True\n",
       "DecisionTree        0.634711    False\n",
       "RandomForest        0.345519    False\n",
       "CatBoost            0.735201    False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constructing a summary table\n",
    "\n",
    "index = ['LogisticRegression',\n",
    "         'DecisionTree',\n",
    "         'RandomForest',\n",
    "         'CatBoost']\n",
    "final_data = {'F1':[round(lr_f1_up, 3),\n",
    "                      best_result_dt_f1,\n",
    "                      best_result_rf_f1,\n",
    "                      f1_catboost_valid]}\n",
    "final_data_table = pd.DataFrame(data=final_data, index=index)\n",
    "final_data_table['Success'] = final_data_table['F1'] > 0.75\n",
    "\n",
    "print('')\n",
    "print('Summary table:')\n",
    "print('')\n",
    "final_data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the Logistic Regression model on the test dataset: 0.7603593161402491\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 14 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# the best result was demonstrated by the Logistic Regression model.\n",
    "# we will test it on the test dataset to ensure it functions properly.\n",
    "predictions_lr_test = model_lr.predict(test_tf_idf)\n",
    "lr_f1_test = f1_score(target_test, predictions_lr_test)\n",
    "print(\"F1 score of the Logistic Regression model on the test dataset:\", lr_f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as a bonus, let's check 3 tweets to see how the model interprets them\n",
    "\n",
    "tweet = [\"It’s exciting to see more & more public figures engaging in active dialogue on this platform!\", \n",
    "         \"The 15 rounds of voting: the US ruling parties weren't electing the House Speaker, but pulling a fat pig.\",\n",
    "         \"Conversations about building a brighter future are essential to driving progress.\"]\n",
    "tweet = count_tf_idf.transform(tweet)\n",
    "model_lr.predict(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>\n",
    "\n",
    "Our goal was to train a model to predict the toxicity of a comment with an F1 quality metric of at least 0.75.\n",
    "\n",
    "In the initial training dataset, a pronounced class imbalance was identified, after which it was determined that the best solution to this problem was upsampling. Text lemmatization was conducted, and the samples were converted to a vector form. Four models were trained - Logistic Regression, Decision Tree, Random Forest, and Cat Boost. We identified a model that meets the client's requirements - with an F1 metric of 0.772 on the validation set and 0.760 on the test set.\n",
    "\n",
    "Thus, the task set before us has been accomplished, and we can recommend the Logistic Regression model to the client."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
